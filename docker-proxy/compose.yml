services:
  watchtower:
    container_name: watchtower
    hostname: ${WEBDOMAIN}
    image: containrrr/watchtower:latest
    restart: always
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - TZ=${TZ}
      - WATCHTOWER_CLEANUP=true
      - WATCHTOWER_INCLUDE_STOPPED=true
      - WATCHTOWER_ROLLING_RESTART=true
      - WATCHTOWER_NOTIFICATION_REPORT=true
      - WATCHTOWER_HEALTHCHECK_GRACE_PERIOD=300
      - WATCHTOWER_POLL_INTERVAL=600
    security_opt:
      - no-new-privileges=true
    networks:
      - traefik-proxy
      - dns-net
    healthcheck:
      test: ["CMD", "/watchtower", "--health-check"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 5s

  logrotate:
    image: blacklabelops/logrotate:1.3
    container_name: logrotate
    restart: unless-stopped
    environment:
      TZ: ${TZ}
      # cron: каждые 5 минут
      LOGROTATE_CRONSCHEDULE: "* */5 * * *"
      # директории с логами (через пробел)
      LOGS_DIRECTORIES: "/logs"
      # какие файлы ротировать (через пробел)
      LOG_FILE_ENDINGS: "log prev.log"
      # максимальный размер для ротации
      LOGROTATE_SIZE: "100k"
      # минимальный размер для ротации
      LOGROTATE_MINSIZE: "50k"
      # количество копий
      LOGROTATE_COPIES: "10"
      # сжимать архивы или нет
      LOGROTATE_COMPRESSION: "nocompress"
      # формат даты в имени архива
      LOGROTATE_DATEFORMAT: "-%Y%m%d-%H%M%S"
      # (опц.) складывать архивы отдельно
      # LOGROTATE_OLDDIR: "/logs/_logrotate/archive"

      # логи самого ротатора/cron
      LOGROTATE_LOGFILE: "/logs/_logrotate/logrotate.log"
      LOG_FILE: "/logs/_logrotate/cron.log"
    volumes:
      - /opt/docker-proxy/_logs:/logs:rw
      - logrotate-status:/logrotate-status


  crowdsec:
    image: crowdsecurity/crowdsec
    container_name: crowdsec
    restart: unless-stopped
    networks:
      - traefik-proxy
      - dns-net
    # dns:
    #   - 172.19.0.10
    ports:
      - "127.0.0.1:8080:8080"
      - "127.0.0.1:7422:7422"
    environment:
      TZ: ${TZ}
      GID: ${GID-1000}
      COLLECTIONS: >
        crowdsecurity/sshd
        crowdsecurity/caddy
        crowdsecurity/http-cve
        crowdsecurity/traefik
        crowdsecurity/linux
        crowdsecurity/whitelist-good-actors
        crowdsecurity/appsec-virtual-patching
        crowdsecurity/appsec-generic-rules
        crowdsecurity/base-http-scenarios
        crowdsecurity/appsec-crs
      BOUNCER_KEY_CADDY: ${CROWDSEC_API_KEY_CADDY}
      BOUNCER_KEY_TRAEFIK: ${CROWDSEC_API_KEY_TRAEFIK}
      BOUNCER_KEY_FIREWALL: ${CROWDSEC_API_KEY_FIREWALL}
    volumes:
      - /opt/docker-proxy/crowdsec/acquis.yaml:/etc/crowdsec/acquis.yaml
      - crowdsec_data:/var/lib/crowdsec/data
      - crowdsec_config:/etc/crowdsec
      - /var/log:/var/log:ro
      - /opt/docker-proxy/_logs:/var/logs:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    security_opt:
      - no-new-privileges=true
    healthcheck:
      test: ["CMD-SHELL", "wget --spider --quiet --tries=1 --timeout=5 http://localhost:8080/health > /dev/null 2>&1 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 10s

  crowdsec-firewall-bouncer:
    image: ghcr.io/shgew/cs-firewall-bouncer-docker:latest
    container_name: crowdsec_firewall_bouncer
    network_mode: host
    depends_on:
      crowdsec:
        condition: service_healthy
    cap_add:
      - NET_ADMIN
      - NET_RAW
    security_opt:
      - no-new-privileges:true
    environment:
      API_KEY: ${CROWDSEC_API_KEY_FIREWALL}
      CROWDSEC_API: 127.0.0.1:8080
    volumes:
      - /opt/docker-proxy/_logs/crowdsec/firewall-bouncer.log:/var/log/crowdsec-firewall-bouncer.log
      - /opt/docker-proxy/crowdsec/crowdsec-firewall-bouncer.yaml:/config/crowdsec-firewall-bouncer.yaml:ro
      - /etc/localtime:/etc/localtime:ro
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "pgrep", "-f", "crowdsec-firewall-bouncer"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  caddy:
    image: torotin/caddy-l4:latest
    container_name: caddy
    restart: unless-stopped
    depends_on:
      crowdsec:
        condition: service_healthy
    mem_limit: 512m
    cpus: "0.5"
    hostname: ${WEBDOMAIN}
    networks:
      - traefik-proxy
      - dns-net
    # dns:
    #   - 172.19.0.10
    volumes:
      - /opt/docker-proxy/_logs/caddy:/var/log/caddy
      - /opt/docker-proxy/caddy/config:/etc/caddy
      - /opt/docker-proxy/caddy/subs:/srv/subs:ro
      # - /opt/docker-proxy/caddy/site:/srv
    env_file:
      - /opt/docker-proxy/.env
    security_opt:
      - no-new-privileges=true
    # labels:
    #   - traefik.enable=true
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:2019/config | grep -q '\"apps\"' || nc -z 127.0.0.1 ${PORT_LOCAL_CADDYWEB}"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 45s
      
  traefik:
    image: traefik:latest
    container_name: traefik
    restart: unless-stopped
    depends_on:
      crowdsec:
        condition: service_healthy
      caddy:
        condition: service_healthy
      dozzle:
        condition: service_healthy
    env_file:
      - /opt/docker-proxy/.env
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /opt/docker-proxy/traefik/traefik.yml:/etc/traefik/traefik.yml:ro
      - /opt/docker-proxy/traefik/entrypoint.sh:/entrypoint.sh:ro
      - /opt/docker-proxy/traefik/acme.json:/acme.json
      - /opt/docker-proxy/traefik/dynamic:/templates:ro
      - /opt/docker-proxy/_logs/traefik:/logs
    ports:
      - "80:80"
      - "443:443/tcp"
      - "443:443/udp"
    entrypoint:
      - /bin/sh
      - -c
      - |
        exec /entrypoint.sh "$@"
    networks:
      - traefik-proxy
      - dns-net
    # dns:
    #   - 172.19.0.10
    healthcheck:
      test: ["CMD-SHELL", "pgrep -x traefik >/dev/null && nc -z 127.0.0.1 80 && nc -z 127.0.0.1 443"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 20s

  dozzle:
    container_name: dozzle
    image: amir20/dozzle:latest
    restart: always
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    env_file:
      - /opt/docker-proxy/.env
    environment:
      - DOZZLE_ENABLE_ACTIONS=true
      - DOZZLE_BASE=/${URI_DOZZLE}
    healthcheck:
      test: ["CMD", "/dozzle", "healthcheck"]
      interval: 3s
      timeout: 30s
      retries: 5
      start_period: 30s
    networks:
      - traefik-proxy
      - dns-net
    # dns:
    #   - 172.19.0.10
    security_opt:
      - no-new-privileges=true

  3x-ui:
    image: torotin/3x-ui:latest
    container_name: 3x-ui
    hostname: ${WEBDOMAIN}
    restart: always
    depends_on:
      traefik:
        condition: service_healthy
      warp:
        condition: service_healthy
      adguard:
        condition: service_healthy
      tor-proxy:
        condition: service_healthy
    tty: true
    security_opt:
      - no-new-privileges=true
    networks:
      traefik-proxy:
      dns-net:
    dns:
      - 172.19.0.10
    volumes:
      - /opt/docker-proxy/3x-ui/scripts/00_BeforeStart:/mnt/sh/beforestart
      - /opt/docker-proxy/3x-ui/scripts/01_AfterStart:/mnt/sh/afterstart
      - /opt/docker-proxy/3x-ui/configs/x-ui.db:/etc/x-ui/x-ui.db:rw
      - /opt/docker-proxy/3x-ui/configs/config.json:/app/bin/config.json:rw
      - /opt/docker-proxy/_logs/xray/access.log:/app/access.log
      - /opt/docker-proxy/_logs/xray/error.log:/app/error.log
      - /opt/docker-proxy/_logs:/var/log:rw
    env_file:
      - /opt/docker-proxy/.env
      - /opt/docker-proxy/3x-ui/3x-ui.env
    environment:
      # - XUI_LOG_LEVEL=debug
      - XUI_LOG_FOLDER=/var/log/3x-ui
    device_cgroup_rules:
      - 'c 10:200 rwm'
    cap_add:
      - NET_ADMIN
    sysctls:
      - net.ipv6.conf.all.disable_ipv6=0
      - net.ipv4.conf.all.src_valid_mark=1
    healthcheck:
      # 1) проверяем, что панель жива по admin-порту (PORT_LOCAL_VLESS_PANEL или 2053 по умолчанию)
      # 2) если панель отключена, хотя бы убедимся, что процесс жив (pgrep)
      test: ["CMD-SHELL", "curl -fsS 127.0.0.1:$PORT_LOCAL_VLESS_PANEL/$URI_PANEL_PATH/ >/dev/null || nc -z 127.0.0.1 ${PORT_LOCAL_VISION} || pgrep -f 'x-ui|xray|sing-box' >/dev/null"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 45s
      
  warp:
    image: caomingjun/warp
    container_name: warp
    restart: always
    networks:
      - traefik-proxy
      - dns-net
    devices:
      - /dev/net/tun:/dev/net/tun
    device_cgroup_rules:
      - 'c 10:200 rwm'
    cap_add:
      - NET_ADMIN
    sysctls:
      net.ipv6.conf.all.disable_ipv6: 0
      net.ipv4.conf.all.src_valid_mark: 1
    environment:
      - WARP_SLEEP=5
    # volumes:
    #   - /opt/docker-proxy/warp/data:/var/lib/cloudflare-warp
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS -x socks5h://127.0.0.1:1080 https://cloudflare.com/cdn-cgi/trace | grep -E 'warp=(on|plus)'"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    # security_opt:
    #   - no-new-privileges=true

  docker-warp-socks:
    image: ghcr.io/mon-ius/docker-warp-socks:v5
    container_name: warp_socks_v5
    restart: unless-stopped
    networks:
      - traefik-proxy
      - dns-net
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS -x socks5h://127.0.0.1:9091 https://cloudflare.com/cdn-cgi/trace | grep -E 'warp=(on|plus)'"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    security_opt:
      - no-new-privileges=true

  adguard:
    image: adguard/adguardhome:latest
    container_name: adguard
    restart: unless-stopped
    networks:
      traefik-proxy:
      dns-net:
        ipv4_address: 172.19.0.10
    # DNS service ports (uncomment if 53 is free on host)
    # ports:
    #   - "53:53/udp"
    #   - "53:53/tcp"
    volumes:
      - /opt/docker-proxy/adguard/work:/opt/adguardhome/work
      - /opt/docker-proxy/adguard/conf:/opt/adguardhome/conf
      - /opt/docker-proxy/adguard/update-pass.sh:/opt/adguardhome/update-pass.sh:ro
    env_file:
      - /opt/docker-proxy/.env
    security_opt:
      - no-new-privileges=true
    entrypoint:
      - /bin/sh
      - -c
      - |
        set -e
        apk add --no-cache bind-tools curl
        if [ -f /opt/adguardhome/update-pass.sh ]; then
          sh /opt/adguardhome/update-pass.sh || echo "[adguard-entrypoint] password update skipped"
        fi
        exec /opt/adguardhome/AdGuardHome -s run --config /opt/adguardhome/conf/AdGuardHome.yaml --work-dir /opt/adguardhome/work
    healthcheck:
      test: ["CMD-SHELL", "dig @127.0.0.1 -p 53 example.com +short | grep -q '.'"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  tor-proxy:
    container_name: tor-proxy
    image: peterdavehello/tor-socks-proxy:latest
    restart: unless-stopped
    # ports:
    ## SOCKS
    #   - 127.0.0.1:9150:9150/tcp
    ## DNS
    #   - 127.0.0.1:8853:8853/tcp
    healthcheck:
      test: >
        curl --fail --silent --socks5-hostname tor-proxy:9150 https://check.torproject.org/api/ip
        | grep '"IsTor":true'
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    networks:
      - traefik-proxy
      - dns-net
    # dns:
    #   - 172.19.0.10
    security_opt:
      - no-new-privileges=true
      
  sub2sing:
    image: ghcr.io/legiz-ru/sub2sing-box:latest
    container_name: sub2sing
    restart: unless-stopped
    extra_hosts:
      - ${WEBDOMAIN}:${PUBLIC_IPV4}
    networks:
      - traefik-proxy
      - dns-net
    # dns:
    #   - 172.19.0.10
      
networks:
  traefik-proxy:
    external: true
  dns-net:
    internal: true
    ipam:
      config:
        - subnet: 172.31.0.0/24

volumes:
  crowdsec_data:
    driver: local
  crowdsec_config:
    driver: local
  logrotate-status:
    driver: local
